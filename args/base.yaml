# Base configuration for MultiCoCo

# Project Configuration
debug: false
project: "multi"
seed: 42

# Training Configuration
batch_size: 8
eval_batch_size: 16
gradient_accumulation_steps: 4
eval_accumulation_steps: 1
learning_rate: 1.0e-5
weight_decay: 0.01
warmup_steps: 500
max_grad_norm: 1.0
lr_scheduler_type: "cosine"
limit_for_testing: false
resume_from_checkpoint: false
load_best_model_at_end: true
metric_for_best_model: "accuracy"
greater_is_better: true

# Evaluation Strategy Configuration
# skip_eval_during_training: false  # Set to true to skip all evaluations between epochs
# eval_strategy: "epoch"             # Options: "epoch", "steps", "no"
save_strategy: "epoch"             # Options: "epoch", "steps", "no"

# Generation Configuration
generation:
  do_sample: true
  max_new_tokens: 256
  num_beams: 1
  temperature: 0.7
  top_p: 0.9
  top_k: 50

# Model Configuration
bf16: true
fp16: false
model_name: "OpenGVLab/InternVL3-1B-Pretrained"
torch_compile: true
use_flash_attention_2: false
# gradient_checkpointing: true
gradient_checkpointing_kwargs: {"use_reentrant": false}
dataloader_num_workers: 8

# Data Configuration
data_dir: "data/"
eval_data_path: "data/aokvqa_validation.json"
train_data_path: "data/aokvqa_train.json"

# Logging Configuration
logging:
  log_dir: "logs/"
  log_level: "INFO"
  use_wandb: false
  run_name: null
  log_to_file: true
  console_output: true
  verbose: false

# Checkpoint Management
keep_best_checkpoints: true
max_checkpoints_to_keep: 30
use_run_name_in_output_dir: true