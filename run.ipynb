{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174a0f1f-d61e-4818-9133-b62a9936cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"shivang2004jhalani@gmail.com\"\n",
    "!git config --global user.name \"shivangjhalani\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c204c31-d1d6-4e78-9817-919530a26e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 2 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"ALL WORKING\"\n",
    "!echo 'shivangjhalani\\n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\\n' | git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca0d842a-105c-45b2-a3e3-f607bdf68389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n",
      "fatal: destination path 'multicoco' already exists and is not an empty directory.\n",
      "/home/ubuntu/multicoco\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (4/4), 610 bytes | 152.00 KiB/s, done.\n",
      "From https://github.com/shivangjhalani/multicoco\n",
      "   7d51e5b..6e48929  main       -> origin/main\n",
      "Updating 7d51e5b..6e48929\n",
      "Fast-forward\n",
      " multicoco/latent_wrapper.py | 18 \u001b[32m++++++++++\u001b[m\u001b[31m--------\u001b[m\n",
      " 1 file changed, 10 insertions(+), 8 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "!git clone https://github.com/shivangjhalani/multicoco.git\n",
    "%cd multicoco\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feb12fe3-17f4-404b-9b70-884aef14d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LatentWrapper state_dict shared memory fix\n",
      "This test verifies that the embedding consistency fix doesn't cause shared memory errors\n",
      "==================================================\n",
      "Testing state_dict shared memory fix...\n",
      "==================================================\n",
      "Creating mock model and tokenizer...\n",
      "Creating LatentWrapper...\n",
      "INFO:multicoco.latent_wrapper:Using original embedding layer to maintain consistent embedding space across CoCoNut passes\n",
      "\n",
      "1. Testing embedding reference consistency...\n",
      "INFO:multicoco.latent_wrapper:Using original embedding layer to maintain consistent embedding space across CoCoNut passes\n",
      "âœ“ Embedding is correctly referencing base model embedding\n",
      "\n",
      "2. Testing state_dict for shared memory...\n",
      "/home/ubuntu/multicoco/multicoco/latent_wrapper.py:1055: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  module.state_dict(destination, prefix + name + '.', keep_vars)\n",
      "Non-base_model embedding keys in state_dict: []\n",
      "âœ“ No duplicate embedding keys found in state_dict\n",
      "\n",
      "3. Testing for shared memory manually...\n",
      "/home/ubuntu/multicoco/test_state_dict_fix.py:94: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_ptr = tensor.storage().data_ptr()\n",
      "âœ“ No shared memory detected in state_dict\n",
      "\n",
      "4. Testing save/load cycle...\n",
      "Saving state_dict to /tmp/tmpudnh5wse/test_model.pt...\n",
      "âœ“ State dict saved successfully\n",
      "Loading state_dict...\n",
      "INFO:multicoco.latent_wrapper:Using original embedding layer to maintain consistent embedding space across CoCoNut passes\n",
      "INFO:multicoco.latent_wrapper:Using original embedding layer to maintain consistent embedding space across CoCoNut passes\n",
      "INFO:multicoco.latent_wrapper:Reinitialized embedding layer after state_dict loading to maintain consistency\n",
      "âœ“ State dict loaded successfully\n",
      "\n",
      "5. Testing embedding consistency after load...\n",
      "INFO:multicoco.latent_wrapper:Using original embedding layer to maintain consistent embedding space across CoCoNut passes\n",
      "âœ“ Loaded wrapper has correct embedding reference\n",
      "\n",
      "==================================================\n",
      "Manual shared memory detection test...\n",
      "==================================================\n",
      "INFO:multicoco.latent_wrapper:Using original embedding layer to maintain consistent embedding space across CoCoNut passes\n",
      "Checking for shared memory in state_dict...\n",
      "âœ“ No shared memory detected in state_dict\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! ðŸŽ‰\n",
      "The state_dict fix successfully resolves shared memory issues\n",
      "while maintaining embedding consistency.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python test_state_dict_fix.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63425a-ed96-47fd-8842-d6ea8104cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fc140-77a9-4e09-87d4-b0b151183787",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login f52d8e8c2c53cf9f55b59cfb98ad26ef47efead9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d15f3-40f1-4815-b822-dfd9db5c6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocessing/aokvqa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfce6a3-a183-442e-9a89-2fdf8d90a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd checkpoints\n",
    "!huggingface-cli download ThefirstM/checkpoints   --include \"aokvqa_cot_aokvqa-cot-stage0/epoch-8/*\"   --local-dir .   --local-dir-use-symlinks False\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a968538-3bda-4aa0-83dd-e33bf6d9935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal model OpenGVLab/InternVL3-1B-Pretrained detected but no image_processor_id specified. Using model_name as fallback.\n",
      "Large batch size (8) with multimodal model may cause OOM. Consider reducing batch_size.\n",
      "INFO - Logging initialized. Output saved to: logs/aokvqa-vanilla-eval_20250714-105519\n",
      "INFO - CUDA available with 1 devices (deterministic mode)\n",
      "INFO - MultiCoCoRunner initialized for evaluation\n",
      "INFO - VANILLA evaluation - no latent tokens added\n",
      "INFO - Loading base model: OpenGVLab/InternVL3-1B-Pretrained\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v2\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 12\n",
      "INFO - vision_config is None. Initializing the InternVisionConfig with default values.\n",
      "INFO - llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v1\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 6\n",
      "FlashAttention2 is not installed.\n",
      "INFO - num_image_token: 256\n",
      "INFO - ps_version: v2\n",
      "INFO - No special tokens added - using base tokenizer vocabulary\n",
      "INFO - Vision-text dimension mismatch: vision=1024, text=896. Using projector.\n",
      "INFO - No embedding resize needed - tokenizer size 151674 matches model vocab size 151674\n",
      "INFO - MultiCoCo model initialized with 938193024 parameters\n",
      "INFO - Model initialized from base model: OpenGVLab/InternVL3-1B-Pretrained\n",
      "INFO - Dtype: bfloat16, BF16: True, FP16: False\n",
      "INFO - Mode: eval_only, CoCoNut: False\n",
      "INFO - Limited dataset to 20 samples for testing\n",
      "INFO - Loaded 20 samples from /home/ubuntu/multicoco/data/aokvqa_validation.json\n",
      "INFO - Evaluation dataset: 20 samples\n",
      "INFO - Starting evaluation only...\n",
      "INFO - CoCoTrainer initialized.\n",
      "INFO - Trainer created successfully\n",
      "INFO - Starting evaluation...\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.68s/it]\n",
      "INFO - Latency Metrics: Avg=0.3675s, Min=0.1988s, Max=1.0656s, Total=7.35s\n",
      "INFO - EVAL METRICS: {'eval_accuracy': 0.65, 'eval_num_samples': 20, 'eval_correct': 13, 'eval/avg_latency_sec': 0.367469584941864, 'eval/min_latency_sec': 0.198836088180542, 'eval/max_latency_sec': 1.0655834674835205, 'eval/total_eval_time_sec': 7.34939169883728}\n",
      "INFO - Logging 20 per-sample evaluation details to file...\n",
      "INFO - \n",
      "==================================================\n",
      "INFO - EVALUATION SUMMARY\n",
      "INFO - ==================================================\n",
      "INFO -   eval_accuracy: 0.6500\n",
      "INFO -   eval_num_samples: 20.0000\n",
      "INFO -   eval_correct: 13.0000\n",
      "INFO -   eval/avg_latency_sec: 0.3675\n",
      "INFO -   eval/min_latency_sec: 0.1988\n",
      "INFO -   eval/max_latency_sec: 1.0656\n",
      "INFO -   eval/total_eval_time_sec: 7.3494\n",
      "INFO - ==================================================\n",
      "INFO - MultiCoCo runner cleanup complete\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS\n",
      "==================================================\n",
      "eval_accuracy: 0.65\n",
      "eval_num_samples: 20\n",
      "eval_correct: 13\n",
      "eval/avg_latency_sec: 0.367469584941864\n",
      "eval/min_latency_sec: 0.198836088180542\n",
      "eval/max_latency_sec: 1.0655834674835205\n",
      "eval/total_eval_time_sec: 7.34939169883728\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 1 run.py args/aokvqa_vanilla_eval.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc22c7c-6ee0-44b8-a871-916d69c163c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal model OpenGVLab/InternVL3-1B-Pretrained detected but no image_processor_id specified. Using model_name as fallback.\n",
      "Large batch size (16) with multimodal model may cause OOM. Consider reducing batch_size.\n",
      "INFO - Logging initialized. Output saved to: logs/aokvqa-cot-stage0_20250714-105542\n",
      "INFO - CUDA available with 1 devices (deterministic mode)\n",
      "INFO - MultiCoCoRunner initialized for training\n",
      "INFO - CoT training phase - no latent tokens added\n",
      "INFO - Loading base model: OpenGVLab/InternVL3-1B-Pretrained\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v2\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 12\n",
      "INFO - vision_config is None. Initializing the InternVisionConfig with default values.\n",
      "INFO - llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v1\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 6\n",
      "FlashAttention2 is not installed.\n",
      "INFO - num_image_token: 256\n",
      "INFO - ps_version: v2\n",
      "INFO - No special tokens added - using base tokenizer vocabulary\n",
      "INFO - Vision-text dimension mismatch: vision=1024, text=896. Using projector.\n",
      "INFO - No embedding resize needed - tokenizer size 151674 matches model vocab size 151674\n",
      "INFO - MultiCoCo model initialized with 938193024 parameters\n",
      "INFO - Model initialized from base model: OpenGVLab/InternVL3-1B-Pretrained\n",
      "INFO - Dtype: bfloat16, BF16: True, FP16: False\n",
      "INFO - Mode: cot_train, CoCoNut: False\n",
      "INFO - Loaded 17056 samples from /home/ubuntu/multicoco/data/aokvqa_train.json\n",
      "INFO - Training dataset: 17056 samples\n",
      "INFO - Loaded 1145 samples from /home/ubuntu/multicoco/data/aokvqa_validation.json\n",
      "INFO - Evaluation dataset: 1145 samples\n",
      "INFO - Starting CoT training...\n",
      "WARNING - Conflicting configuration detected: skip_eval_during_training=True but load_best_model_at_end=True. Cannot load best model without evaluations. Consider setting load_best_model_at_end=False.\n",
      "INFO - CoCoTrainer initialized.\n",
      "INFO - Trainer created successfully\n",
      "INFO - Starting training...\n",
      "INFO - Training state initialized for epoch-based training\n",
      "INFO - Starting epoch-based training:\n",
      "INFO -   Steps per epoch: 533\n",
      "INFO -   Total epochs: 8\n",
      "INFO -   Total steps: 4264\n",
      "INFO - Created cosine scheduler with warmup_steps=500, total_steps=4264\n",
      "INFO - Optimizer created with initial LR: 0.0\n",
      "INFO - Scheduler: LambdaLR, num_training_steps=4264\n",
      "INFO - Warmup steps: 500\n",
      "INFO - Gradient clipping enabled with max_grad_norm: 1.0\n",
      "INFO - \n",
      "Starting Epoch 1/8\n",
      "Epoch 1:   0%|                                         | 0/1066 [00:00<?, ?it/s]/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1:   0%|                                         | 0/1066 [00:02<?, ?it/s]\n",
      "INFO - MultiCoCo runner cleanup complete\n",
      "Error: CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 23.55 GiB of which 235.38 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 16.71 GiB is allocated by PyTorch, and 678.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "E0714 10:55:52.722000 11358 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 11423) of binary: /home/ubuntu/my_jupyter_projects/jupyter_env/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in main\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/run.py\", line 883, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "run.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-14_10:55:52\n",
      "  host      : vacant-lemon-butterfly-8587978c85-2qxlm\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 11423)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 1 run.py args/aokvqa_cot.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f86f67a-5785-46b4-a5a0-dff49e2b2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal model OpenGVLab/InternVL3-1B-Pretrained detected but no image_processor_id specified. Using model_name as fallback.\n",
      "Large batch size (8) with multimodal model may cause OOM. Consider reducing batch_size.\n",
      "INFO - Logging initialized. Output saved to: logs/aokvqa-cot-eval_20250714-105600\n",
      "INFO - CUDA available with 1 devices (deterministic mode)\n",
      "INFO - MultiCoCoRunner initialized for evaluation\n",
      "INFO - COT evaluation - no latent tokens added\n",
      "INFO - Loading from checkpoint: checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v2\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 12\n",
      "INFO - vision_config is None. Initializing the InternVisionConfig with default values.\n",
      "INFO - llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v1\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 6\n",
      "FlashAttention2 is not installed.\n",
      "INFO - num_image_token: 256\n",
      "INFO - ps_version: v2\n",
      "INFO - No special tokens added - using base tokenizer vocabulary\n",
      "INFO - Vision-text dimension mismatch: vision=1024, text=896. Using projector.\n",
      "INFO - No embedding resize needed - tokenizer size 151674 matches model vocab size 151674\n",
      "INFO - MultiCoCo model initialized with 938193024 parameters\n",
      "INFO - Loading checkpoint from safetensors: checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8/model.safetensors\n",
      "INFO - Successfully loaded model weights from checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8/model.safetensors\n",
      "INFO - Model initialized from checkpoint: checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8\n",
      "INFO - Dtype: bfloat16, BF16: True, FP16: False\n",
      "INFO - Mode: eval_only, CoCoNut: False\n",
      "INFO - Limited dataset to 20 samples for testing\n",
      "INFO - Loaded 20 samples from /home/ubuntu/multicoco/data/aokvqa_validation.json\n",
      "INFO - Evaluation dataset: 20 samples\n",
      "INFO - Starting evaluation only...\n",
      "INFO - CoCoTrainer initialized.\n",
      "INFO - Trainer created successfully\n",
      "INFO - Starting evaluation...\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.47s/it]\n",
      "INFO - Latency Metrics: Avg=1.3880s, Min=0.5677s, Max=2.3002s, Total=27.76s\n",
      "INFO - EVAL METRICS: {'eval_accuracy': 0.75, 'eval_num_samples': 20, 'eval_correct': 15, 'eval/avg_latency_sec': 1.3879524350166321, 'eval/min_latency_sec': 0.5677111148834229, 'eval/max_latency_sec': 2.3001773357391357, 'eval/total_eval_time_sec': 27.75904870033264}\n",
      "INFO - Logging 20 per-sample evaluation details to file...\n",
      "INFO - \n",
      "==================================================\n",
      "INFO - EVALUATION SUMMARY\n",
      "INFO - ==================================================\n",
      "INFO -   eval_accuracy: 0.7500\n",
      "INFO -   eval_num_samples: 20.0000\n",
      "INFO -   eval_correct: 15.0000\n",
      "INFO -   eval/avg_latency_sec: 1.3880\n",
      "INFO -   eval/min_latency_sec: 0.5677\n",
      "INFO -   eval/max_latency_sec: 2.3002\n",
      "INFO -   eval/total_eval_time_sec: 27.7590\n",
      "INFO - ==================================================\n",
      "INFO - MultiCoCo runner cleanup complete\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS\n",
      "==================================================\n",
      "eval_accuracy: 0.75\n",
      "eval_num_samples: 20\n",
      "eval_correct: 15\n",
      "eval/avg_latency_sec: 1.3879524350166321\n",
      "eval/min_latency_sec: 0.5677111148834229\n",
      "eval/max_latency_sec: 2.3001773357391357\n",
      "eval/total_eval_time_sec: 27.75904870033264\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 1 run.py args/aokvqa_cot_eval.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ea6abec-5fd1-49b5-83c8-991728d75e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal model OpenGVLab/InternVL3-1B-Pretrained detected but no image_processor_id specified. Using model_name as fallback.\n",
      "Large batch size (8) with multimodal model may cause OOM. Consider reducing batch_size.\n",
      "INFO - Logging initialized. Output saved to: logs/aokvqa-coconut-multistage_20250714-111006\n",
      "INFO - CUDA available with 1 devices (deterministic mode)\n",
      "INFO - MultiCoCoRunner initialized for training\n",
      "INFO - Adding latent special tokens: ['<|start_latent|>', '<|end_latent|>', '<|latent|>']\n",
      "INFO - Loading from checkpoint: checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v2\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 12\n",
      "INFO - vision_config is None. Initializing the InternVisionConfig with default values.\n",
      "INFO - llm_config is None. Initializing the LlamaConfig config with default values (`LlamaConfig`).\n",
      "INFO - vision_select_layer: -1\n",
      "INFO - ps_version: v1\n",
      "INFO - min_dynamic_patch: 1\n",
      "INFO - max_dynamic_patch: 6\n",
      "FlashAttention2 is not installed.\n",
      "INFO - num_image_token: 256\n",
      "INFO - ps_version: v2\n",
      "INFO - Added 3 special tokens: ['<|start_latent|>', '<|end_latent|>', '<|latent|>']\n",
      "INFO - Vision-text dimension mismatch: vision=1024, text=896. Using projector.\n",
      "INFO - Resizing embeddings from 151674 to 151677 for 3 new tokens\n",
      "INFO - MultiCoCo model initialized with 938198400 parameters\n",
      "INFO - Loading checkpoint from safetensors: checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8/model.safetensors\n",
      "WARNING - Vocabulary size mismatch: checkpoint=151674, current=151677\n",
      "INFO - Handling vocabulary size mismatch by resizing embeddings...\n",
      "INFO - Temporarily resized model embeddings to 151674 to match checkpoint\n",
      "INFO - Resized model embeddings back to current vocab size: 151677\n",
      "INFO - New token embeddings will be randomly initialized\n",
      "INFO - Successfully loaded model weights from checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8/model.safetensors\n",
      "INFO - Using original embedding layer to maintain consistent embedding space across CoCoNut passes\n",
      "INFO - Model initialized from checkpoint: checkpoints/aokvqa_cot_aokvqa-cot-stage0/epoch-8\n",
      "INFO - Dtype: bfloat16, BF16: True, FP16: False\n",
      "INFO - Mode: coconut_train, CoCoNut: True\n",
      "INFO - Limited dataset to 20 samples for testing\n",
      "INFO - Loaded 20 samples from /home/ubuntu/multicoco/data/aokvqa_train.json\n",
      "INFO - Training dataset: 20 samples\n",
      "INFO - Limited dataset to 20 samples for testing\n",
      "INFO - Loaded 20 samples from /home/ubuntu/multicoco/data/aokvqa_validation.json\n",
      "INFO - Evaluation dataset: 20 samples\n",
      "INFO - Starting CoCoNut multi-stage training...\n",
      "INFO - CoCoTrainer initialized.\n",
      "INFO - Trainer created successfully\n",
      "INFO - Starting CoCoNut multi-stage training with stage transitions\n",
      "INFO - Training state initialized for epoch-based training\n",
      "INFO - Starting epoch-based training:\n",
      "INFO -   Steps per epoch: 1\n",
      "INFO -   Total epochs: 24\n",
      "INFO -   Total steps: 24\n",
      "INFO - Created cosine scheduler with warmup_steps=500, total_steps=24\n",
      "INFO - Optimizer created with initial LR: 0.0\n",
      "INFO - Scheduler: LambdaLR, num_training_steps=24\n",
      "INFO - Warmup steps: 500\n",
      "INFO - Gradient clipping enabled with max_grad_norm: 1.0\n",
      "INFO - Transitioning to CoCoNut stage 0\n",
      "INFO - Dataset sample before curriculum update (stage 0): steps=['A train would not be on the street, he would not have luggage waiting for a delivery, and the skateboarder is there and not paying attention to him so a cab is the only possible answer.', 'He has bags as if he is going someone, and he is on a road waiting for vehicle that can only be moved on the road and is big enough to hold the bags.', 'He looks to be waiting for a paid ride to pick him up.']\n",
      "INFO - Applying progressive curriculum for stage 0\n",
      "INFO - Creating progressive latent dataset for stage 0\n",
      "INFO - Dataset updated with 20 curriculum samples\n",
      "INFO - Dataset sample after curriculum update (stage 0): steps=['A train would not be on the street, he would not have luggage waiting for a delivery, and the skateboarder is there and not paying attention to him so a cab is the only possible answer.', 'He has bags as if he is going someone, and he is on a road waiting for vehicle that can only be moved on the road and is big enough to hold the bags.', 'He looks to be waiting for a paid ride to pick him up.']\n",
      "INFO - Applied progressive curriculum for stage 0 - Dataset size: 20\n",
      "INFO - Dataloader will be refreshed for updated curriculum\n",
      "INFO - Resetting optimizer and scheduler for new stage\n",
      "INFO - Created cosine scheduler with warmup_steps=500, total_steps=24\n",
      "INFO - Optimizer created with initial LR: 0.0\n",
      "INFO - Scheduler: LambdaLR, num_training_steps=24\n",
      "INFO - Warmup steps: 500\n",
      "INFO - Gradient clipping enabled with max_grad_norm: 1.0\n",
      "INFO - Reset optimizer and scheduler with 24 remaining training steps\n",
      "INFO - Epoch 1/24 - CoCoNut Stage 0/6 (Stage Epoch 1/3)\n",
      "INFO - \n",
      "Starting Epoch 1/24\n",
      "Epoch 1:   0%|                                            | 0/3 [00:00<?, ?it/s]/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 1/3 [00:02<00:05,  2.59s/it]/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "[rank0]:[W714 11:10:39.961068597 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.31s/it, loss=0.6406, lr=0.000000]\n",
      "INFO - Epoch 1 training complete. Average loss: 0.6979\n",
      "INFO - Running evaluation after epoch 1...\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.36s/it]\n",
      "INFO - Latency Metrics: Avg=1.3164s, Min=0.6045s, Max=2.1296s, Total=26.33s\n",
      "INFO - EVAL METRICS: {'eval_accuracy': 0.65, 'eval_num_samples': 20, 'eval_correct': 13, 'eval/avg_latency_sec': 1.316442608833313, 'eval/min_latency_sec': 0.6045198440551758, 'eval/max_latency_sec': 2.129584312438965, 'eval/total_eval_time_sec': 26.32885217666626}\n",
      "INFO - Logging 20 per-sample evaluation details to file...\n",
      "/home/ubuntu/multicoco/multicoco/latent_wrapper.py:1055: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  module.state_dict(destination, prefix + name + '.', keep_vars)\n",
      "INFO - Checkpoint saved with metrics: checkpoints/aokvqa_coconut_aokvqa-coconut-multistage/epoch-1\n",
      "INFO - \n",
      "EPOCH 1 SUMMARY\n",
      "INFO - Checkpoint: checkpoints/aokvqa_coconut_aokvqa-coconut-multistage/epoch-1\n",
      "INFO - Epoch time: 65.64s\n",
      "INFO - Evaluation metrics:\n",
      "INFO -   eval_accuracy: 0.6500\n",
      "INFO -   eval_num_samples: 20.0000\n",
      "INFO -   eval_correct: 13.0000\n",
      "INFO -   eval/avg_latency_sec: 1.3164\n",
      "INFO -   eval/min_latency_sec: 0.6045\n",
      "INFO -   eval/max_latency_sec: 2.1296\n",
      "INFO -   eval/total_eval_time_sec: 26.3289\n",
      "INFO - Epoch 2/24 - CoCoNut Stage 0/6 (Stage Epoch 2/3)\n",
      "INFO - \n",
      "Starting Epoch 2/24\n",
      "Epoch 2:   0%|                                            | 0/3 [00:01<?, ?it/s]\n",
      "INFO - MultiCoCo runner cleanup complete\n",
      "Error: CUDA out of memory. Tried to allocate 908.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 753.38 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 15.96 GiB is allocated by PyTorch, and 459.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "E0714 11:11:45.512000 12428 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 12494) of binary: /home/ubuntu/my_jupyter_projects/jupyter_env/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in main\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/run.py\", line 883, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ubuntu/my_jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "run.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-14_11:11:45\n",
      "  host      : vacant-lemon-butterfly-8587978c85-2qxlm\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 12494)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 1 run.py args/aokvqa_coconut.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445d0ad-f86b-469f-95dd-ef4ca2214d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
